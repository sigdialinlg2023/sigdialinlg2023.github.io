conference,original_id,UID,title,authors,order,session,paper_id,abstract,paper,notes
inlg,123,inlg123,Overview of MiReportor: Generating Reports for Multimodal Medical Images,Xuwen Wang|Hetong Ma|Zhen Guo|Jiao Li,0,virtualpostersession,inlg123,"This demo paper presents a brief introduction of MiReportor, a computer-aided medical imaging report generator, which leverages a unified framework of medical image understanding and generation to predict readable descriptions for medical images, and assists radiologists in imaging reports writing.",static/papers/inlg/123_Paper.pdf,
inlg,124,inlg124,enunlg: a Python library for reproducible neural data-to-text experimentation,David M. Howcroft|Dimitra Gkatzia,0,genchalpostersession+demos,inlg124,"Over the past decade, a variety of neural architectures for data-to-text generation (NLG) have been proposed. However, each system typically has its own approach to pre- and post-processing and other implementation details. Diversity in implementations is desirable, but it also confounds attempts to compare model performance: are the differences due to the proposed architectures or are they a byproduct of the libraries used or a result of pre- and post-processing decisions made? To improve reproducibility, we re-implement several pre-Transformer neural models for data-to-text NLG within a single framework to facilitate direct comparisons of the models themselves and better understand the contributions of other design choices. We release our library at https://github.com/NapierNLP/enunlg to serve as a baseline for ongoing work in this area including research on NLG for low-resource languages where transformers might not be optimal.",static/papers/inlg/124_Paper.pdf,
inlg,125,inlg125,VisuaLLM: Easy Web-based Visualization for Neural Language Generation,František Trebuňa|Ondrej Dusek,0,genchalpostersession+demos,inlg125,"VisuaLLM is a Python library that enables interactive visualization of common tasks in natural language generation with pretrained language models (using HuggingFace's model API), with tight integration of benchmark datasets and fine-grained generation control. The system runs as a local generation backend server and features a web-based frontend, allowing simple interface configuration by minimal Python code. The currently implemented views include data visualization, next-token prediction with probability distributions, and decoding parameter control, with simple extension to additional tasks.",static/papers/inlg/125_Paper.pdf,
inlg,126,inlg126,Audio Commentary System for Real-Time Racing Game Play,Tatsuya Ishigaki|Goran Topić|Yumi Hamazono|Ichiro Kobayashi|Yusuke Miyao|Hiroya Takamura,0,genchalpostersession+demos,inlg126,"Live commentaries are essential for enhancing spectators' enjoyment and understanding during sports events or e-sports streams. We introduce a live audio commentator system designed specifically for a racing game, driven by the high demand in the e-sports field. While a player is playing a racing game, our system tracks real-time user play data including speed and steer rotations, and generates commentary to accompany the live stream. Human evaluation suggested that generated commentary enhances enjoyment and understanding of races compared to streams without commentary. Incorporating additional modules to improve diversity and detect irregular events, such as course-outs and collisions, further increases the preference for the output commentaries.",static/papers/inlg/126_Paper.pdf,
inlg,9,inlg9,Guided Beam Search to Improve Generalization in Low-Resource Data-to-Text Generation,Nicolas Garneau|Luc Lamontagne,0,inlgoralsession2,inlg9,"In this paper, we introduce a new beam search algorithm that improves the generalization of neural generators to unseen examples, especially in low-resource data-to-text settings. Our algorithm aims to reduce the number of omissions and hallucinations during the decoding process. For this purpose, it relies on two regression models to explicitly characterize factual errors. We explain how to create a new dataset to train these models given an original training set of less than a thousand data points. We apply our approach in the low-resource, legal setting using the French Plum2Text dataset, as well as in English using WebNLG. We observe in our experiment that this combination improves the faithfulness of pre-trained neural text generators using both human and automatic evaluation. Moreover, our approach offers a level of interpretability by predicting the number of omissions and hallucinations present in a given generation with respect to the input data. Finally, we visualize our algorithm's exploration of the hypothesis space at different steps during the decoding process.",static/papers/inlg/9_Paper.pdf,
inlg,10,inlg10,XF2T: Cross-lingual Fact-to-Text Generation for Low-Resource Languages,Shivprasad Sagare|Tushar Abhishek|Bhavyajeet Singh|Anubhav Sharma|Manish Gupta|Vasudeva Varma,0,inlgoralsession2,inlg10,"Multiple business scenarios require an automated generation of descriptive human-readable text from structured input data. This has resulted into substantial work on fact-to-text generation systems recently. Unfortunately, previous work on fact-to-text (F2T) generation has focused primarily on English mainly due to the high availability of relevant datasets. Only recently, the problem of cross-lingual fact-to-text (XF2T) was proposed for generation across multiple languages alongwith a dataset, XAlign for eight languages. However, there has been no rigorous work on the actual XF2T generation problem. We extend XAlign dataset with annotated data for four more languages: Punjabi, Malayalam, Assamese and Oriya. We conduct an extensive study using popular Transformer-based text generation models on our extended multi-lingual dataset, which we call XAlignV2. Further, we investigate the performance of different text generation strategies: multiple variations of pretraining, fact-aware embeddings and structure-aware input encoding. Our extensive experiments show that a multi-lingual mT5 model which uses fact-aware embeddings with structure-aware input encoding leads to best results (30.90 BLEU, 55.12 METEOR and 59.17 chrF++) across the twelve languages. We make our code, dataset and model publicly available, and hope that this will help advance further research in this critical area.",static/papers/inlg/10_Paper.pdf,
inlg,12,inlg12,Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy,Daphne Ippolito|Florian Tramer|Milad Nasr|Chiyuan Zhang|Matthew Jagielski|Katherine Lee|Christopher Choquette Choo|Nicholas Carlini,0,inlgoralsession1,inlg12,"Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data and aids in the development of countermeasures. Many prior works---and some recently deployed defenses---focus on ``verbatim memorization'', defined as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization definitions are too restrictive and fail to capture more subtle forms of memorization. Specifically, we design and implement an efficient defense that _perfectly_ prevents all verbatim memorization.  And yet, we demonstrate that this ``perfect'' filter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and minimally modified ``style-transfer'' prompts---and in some cases even the non-modified original prompts---to extract memorized information. We conclude by discussing potential alternative definitions and why defining memorization is a difficult yet crucial open question for neural language models.",static/papers/inlg/12_Paper.pdf,
inlg,15,inlg15,Fine-Tuning GPT-3 for Synthetic Danish News Generation,Mina Almasi|Anton Schiønning,0,inlgoralsession3,inlg15,"While GPT-3 has garnered significant attention for its capabilities in natural language generation, research on its use outside of English is still relatively limited. We focus on how GPT-3 can be fine-tuned for generating synthetic news articles in a low-resource language, namely Danish. The model's performance is evaluated on the dimensions of human and machine detection in two separate experiments. When presented with either a real or GPT-3 generated news article, human participants achieve a 58.1% classification accuracy. Contrarily, a fine-tuned BERT classifier obtains a 92.7% accuracy on the same task. This discrepancy likely pertains to the fine-tuned GPT-3 model oversampling high-likelihood tokens in its text generation. Although this is undetectable to the human eye, it leaves a statistical discrepancy for machine classifiers to detect. We address how decisions in the experimental design favoured the machine classifiers over the human evaluators, and whether the produced synthetic articles are applicable in a real-world context.",static/papers/inlg/15_Paper.pdf,
inlg,16,inlg16,GAN-LM: Generative Adversarial Network using Language Models for Downstream Applications,Dae Yon Hwang|Yaroslav Nechaev|Cyprien de Lichy|Renxian Zhang,0,inlgoralsession5,inlg16,"In this work, we investigate Data Augmentation methods to improve the performance of state-of-the-art models for four different downstream tasks. Specifically, we propose Generative Adversarial Network using Language Models (GAN-LM) approach that combines a deep generative model with a pre-trained language model to produce diverse augmentations. We compare the GAN-LM to various conventional methods in non-contextual- and contextual-levels on four public datasets: ZESHEL for zero-shot entity linking, TREC for question classification, STS-B for sentence pairs semantic textual similarity (STS), and mSTS for multilingual sentence pairs STS. Additionally, we subsample these datasets to study the impact of such augmentations in low-resource settings where limited amounts of training data is available. Compared to the state-of-the-art methods in downstream tasks, we mostly achieve the best performance using GAN-LM approach. Finally, we investigate the way of combining the GAN-LM with other augmentation methods to complement our proposed approach. The developed code for reproducibility is included in the supplementary material.",static/papers/inlg/16_Paper.pdf,
inlg,24,inlg24,Summaries as Captions: Generating Figure Captions for Scientific Documents with Automated Text Summarization,Chieh-Yang Huang|Ting-Yao Hsu|Ryan Rossi|Ani Nenkova|Sungchul Kim|Gromit Yeuk-Yin Chan|Eunyee Koh|C Lee Giles|Ting-Hao Huang,0,inlgoralsession5,inlg24,"Good figure captions help paper readers understand complex scientific figures. Unfortunately, even published papers often have poorly written captions. Automatic caption generation could aid paper writers by providing good starting captions that can be refined for better quality. Prior work often treated figure caption generation as a vision-to-language task. In this paper, we show that it can be more effectively tackled as a text summarization task in scientific documents. We fine-tuned PEGASUS, a pre-trained abstractive summarization model, to specifically summarize figure-referencing paragraphs (e.g., ""Figure 3 shows..."") into figure captions. Experiments on large-scale arXiv figures show that our method outperforms prior vision methods in both automatic and human evaluations. We further conducted an in-depth investigation focused on two key challenges: (i) the common presence of low-quality author-written captions and (ii) the lack of clear standards for good captions. Our code and data are available at: https://github.com/Crowd-AI-Lab/Generating-Figure-Captions-as-a-Text-Summarization-Task.",static/papers/inlg/24_Paper.pdf,
inlg,30,inlg30,Models of reference production: How do they withstand the test of time?,Fahime Same|Guanyi Chen|Kees van Deemter,0,inlgoralsession4,inlg30,"In recent years, many NLP studies have focused solely on performance improvement. In this work, we focus on the linguistic and scientific aspects of NLP. We use the task of generating referring expressions in context (REG-in-context) as a case study and start our analysis from GREC, a comprehensive set of shared tasks in English that addressed this topic over a decade ago. We ask what the performance of models would be if we assessed them (1) on more realistic datasets, and (2) using more advanced methods. We test the models using different evaluation metrics and feature selection experiments. We conclude that GREC can no longer be regarded as offering a reliable assessment of models' ability to mimic human reference production, because the results are highly impacted by the choice of corpus and evaluation metrics. Our results also suggest that pre-trained language models are less dependent on the choice of corpus than classic Machine Learning models, and therefore make more robust class predictions.",static/papers/inlg/30_Paper.pdf,
inlg,34,inlg34,Generating Faithful Text From a Knowledge Graph with Noisy Reference Text,Tahsina Hashem|Weiqing Wang|Derry Tanti Wijaya|Mohammed Eunus Ali|Yuan-Fang Li,0,inlgoralsession1,inlg34,"Knowledge Graph (KG)-to-Text generation aims at generating fluent natural-language text that accurately represents the information of a given knowledge graph. While significant progress has been made in this task by exploiting the power of pre-trained language models (PLMs) with appropriate graph structure-aware modules, existing models still fall short of generating faithful text, especially when the ground-truth natural-language text contains additional information that is not present in the graph. In this paper, we develop a KG-to-text generation model that can generate faithful natural-language text from a given graph, in the presence of noisy reference text. Our framework incorporates two core ideas: Firstly, we utilize contrastive learning to enhance the model's ability to differentiate between faithful and hallucinated information in the text, thereby encouraging the decoder to generate text that aligns with the input graph. Secondly, we empower the decoder to control the level of hallucination in the generated text by employing a controllable text generation technique. We evaluate our model's performance through the standard quantitative metrics as well as a ChatGPT-based quantitative and qualitative analysis. Our evaluation demonstrates the superior performance of our model over state-of-the-art KG-to-text models on faithfulness.",static/papers/inlg/34_Paper.pdf,
inlg,44,inlg44,Entropy-based Sampling for Abstractive Multi-document Summarization in Low-resource Settings,Laura Mascarell|Ribin Chalumattu|Julien Heitmann,0,inlgoralsession2,inlg44,"Research in Multi-document Summarization (MDS) mostly focuses on the English language and depends on large MDS datasets that are not available for other languages. Some of these approaches concatenate the source documents, resulting in overlong model inputs. Existing transformer architectures are unable to process such long inputs entirely, omitting documents in the summarization process. Other solutions address this issue by implementing multi-stage approaches that also require changes in the model architecture. In this paper, we introduce various sampling approaches based on information entropy that allow us to perform MDS in a single stage. These approaches also consider all source documents without using MDS training data nor changing the model's architecture. Besides, we build a MDS test set of German news articles to assess the performance of our methods on abstractive multi-document summaries. Experimental results show that our entropy-based approaches outperform previous state-of-the-art on German MDS, while still remaining primarily abstractive. We release our code and MDS test set to encourage further research in German abstractive MDS.",static/papers/inlg/44_Paper.pdf,
inlg,47,inlg47,Claim Optimization in Computational Argumentation,Gabriella Skitalinskaya|Maximilian Spliethöver|Henning Wachsmuth,0,inlgoralsession5,inlg47,"An optimal delivery of arguments is key to persuasion in any debate, both for humans and for AI systems. This requires the use of clear and fluent claims relevant to the given debate. Prior work has studied the automatic assessment of argument quality extensively. Yet, no approach actually improves the quality so far. To fill this gap, this paper proposes the task of claim optimization: to rewrite argumentative claims in order to optimize their delivery. As multiple types of optimization are possible, we approach this task by first generating a diverse set of candidate claims using a large language model, such as BART, taking into account contextual information. Then, the best candidate is selected using various quality metrics. In automatic and human evaluation on an English-language corpus, our quality-based candidate selection outperforms several baselines, improving 60% of all claims (worsening 16% only). Follow-up analyses reveal that, beyond copy editing, our approach often specifies claims with details, whereas it adds less evidence than humans do. Moreover, its capabilities generalize well to other domains, such as instructional texts.",static/papers/inlg/47_Paper.pdf,
inlg,56,inlg56,ChatGPT's Information Seeking Strategy: Insights from the 20-Questions Game,Leonardo Bertolazzi|Davide Mazzaccara|Filippo Merlo|Raffaella Bernardi,0,inlgoralsession3,inlg56,"Large Language Models, and ChatGPT in particular, have recently grabbed the attention of the community and the media. Having reached high language proficiency, attention has been shifting toward its reasoning capabilities. In this paper, our main aim is to evaluate ChatGPT's question generation in a task where language production should be driven by an implicit reasoning process. To this end, we employ the 20-Questions game, traditionally used within the Cognitive Science community to inspect the information seeking-strategy's development.  This task requires a series of interconnected skills: asking informative questions, stepwise updating the hypothesis space, and stopping asking questions when enough information has been collected. We build hierarchical hypothesis spaces, exploiting feature norms collected from humans vs. ChatGPT itself, and we inspect the efficiency and informativeness of ChatGPT's strategy. Our results show that ChatGPT's performance gets closer to an optimal agent only when prompted to explicitly list the updated space stepwise.",static/papers/inlg/56_Paper.pdf,
inlg,57,inlg57,This is not correct! Negation-aware Evaluation of Language Generation Systems,Miriam Anschütz|Diego Miguel Lozano|Georg Groh,0,inlgoralsession4,inlg57,"Large language models underestimate the impact of negations on how much they change the meaning of a sentence. Therefore, learned evaluation metrics based on these models are insensitive to negations. In this paper, we propose NegBLEURT, a negation-aware version of the BLEURT evaluation metric. For that, we designed a rule-based sentence negation tool and used it to create the CANNOT negation evaluation dataset. Based on this dataset, we fine-tuned a sentence transformer and an evaluation metric to improve their negation sensitivity. Evaluating these models on existing benchmarks shows that our fine-tuned models outperform existing metrics on the negated sentences by far while preserving their base models' performances on other perturbations.",static/papers/inlg/57_Paper.pdf,
inlg,58,inlg58,Guidance in Radiology Report Summarization: An Empirical Evaluation and Error Analysis,Jan Trienes|Paul Youssef|Jörg Schlötterer|Christin Seifert,0,inlgoralsession1,inlg58,"Automatically summarizing radiology reports into a concise impression can reduce the manual burden of clinicians and improve the consistency of reporting. Previous work aimed to enhance content selection and factuality through guided abstractive summarization. However, two key issues persist. First, current methods heavily rely on domain-specific resources to extract the guidance signal, limiting their transferability to domains and languages where those resources are unavailable. Second, while automatic metrics like ROUGE show progress, we lack a good understanding of the errors and failure modes in this task. To bridge these gaps, we first propose a domain-agnostic guidance signal in form of variable-length extractive summaries. Our empirical results on two English benchmarks demonstrate that this guidance signal improves upon unguided summarization while being competitive with domain-specific methods. Additionally, we run an expert evaluation of four systems according to a taxonomy of 11 fine-grained errors. We find that the most pressing differences between automatic summaries and those of radiologists relate to content selection including omissions (up to 52%) and additions (up to 57%). We hypothesize that latent reporting factors and corpus-level inconsistencies may limit models to reliably learn content selection from the available data, presenting promising directions for future work.",static/papers/inlg/58_Paper.pdf,
inlg,71,inlg71,A Zero-Shot Approach for Multi-User Task-Oriented Dialog Generation,Shiv Surya|Yohan Jo|Arijit Biswas|Alexandros Potamianos,0,inlgoralsession5,inlg71,"Prior art investigating task-oriented dialog and automatic generation of such dialogs have focused on single-user dialogs between a single user and an agent. However, there is limited study on adapting such AI agents to multi-user conversations (involving multiple users and an agent). Multi-user conversations are richer than single-user conversations containing social banter and collaborative decision making. The most significant challenge impeding such studies is the lack of suitable multi-user task-oriented dialogs with annotations of user belief states and system actions. One potential solution is multi-user dialog generation from single-user data. Many single-user dialogs datasets already contain dialog state information (intents, slots), thus making them suitable candidates. In this work, we propose a novel approach for expanding single-user task-oriented dialogs (e.g. MultiWOZ) to multi-user dialogs in a zero-shot setting.",static/papers/inlg/71_Paper.pdf,
inlg,83,inlg83,Beyond the Bias: Unveiling the Quality of Implicit Causality Prompt Continuations in Language Models,Judith Sieker|Oliver Bott|Torgrim Solstad|Sina Zarrieß,0,inlgoralsession4,inlg83,"Recent studies have used human continuations of Implicit Causality (IC) prompts collected in linguistic experiments to evaluate discourse understanding in large language models (LLMs), focusing on the well-known IC coreference bias in the LLMs' predictions of the next word following the prompt. In this study, we investigate how continuations of IC prompts can be used to evaluate the text generation capabilities of LLMs in a linguistically controlled setting. We conduct an experiment using two open-source GPT-based models, employing human evaluation to assess different aspects of continuation quality. Our findings show that LLMs struggle in particular with generating coherent continuations in this rather simple setting, indicating a lack of discourse knowledge beyond the well-known IC bias. Our results also suggest that a bias congruent continuation does not necessarily equate to a higher continuation quality. Furthermore, our study draws upon insights from the Uniform Information Density hypothesis, testing different prompt modifications and decoding procedures and showing that sampling-based methods are particularly sensitive to the information density of the prompts.",static/papers/inlg/83_Paper.pdf,
inlg,97,inlg97,Enhancing factualness and controllability of Data-to-Text Generation via data Views and constraints,Craig Thomson|Clement Rebuffel|Ehud Reiter|Laure Soulier|Somayajulu Sripada|patrick Gallinari,0,inlgoralsession1,inlg97,"Neural data-to-text systems lack the control and factual accuracy required to generate useful and insightful summaries of multidimensional data. We propose a solution in the form of data views, where each view describes an entity and its attributes along specific dimensions. A sequence of views can then be used as a high-level schema for document planning, with the neural model handling the complexities of micro-planning and surface realization. We show that our view-based system retains factual accuracy while offering high-level control of output that can be tailored based on user preference or other norms within the domain.",static/papers/inlg/97_Paper.pdf,
inlg,99,inlg99,Memories for Virtual AI Characters,Fabian Landwehr|Erika Varis Doggett|Romann M. Weber,0,inlgoralsession3,inlg99,"In this paper, we present a system for augmenting virtual AI characters with long-term memory, enabling them to remember facts about themselves, their world, and past experiences. We propose a memory-creation pipeline that converts raw text into condensed memories and a memory-retrieval system that utilizes these memories to generate character responses. Using a fact-checking pipeline based on GPT-4, our evaluation demonstrates that the character responses are grounded in the retrieved memories and maintain factual accuracy. We discuss the implications of our system for creating engaging and consistent virtual characters and highlight areas for future research, including large language model (LLM) guardrailing and virtual character personality development.",static/papers/inlg/99_Paper.pdf,
inlg,107,inlg107,Metric-Based In-context Learning: A Case Study in Text Simplification,Subhadra Vadlamannati|Gözde Şahin,0,inlgoralsession3,inlg107,"In-context learning (ICL) for large language models has proven to be a powerful approach for many natural language processing tasks. However, determining the best method to select examples for ICL is nontrivial as the results can vary greatly depending on the quality, quantity, and order of examples used. In this paper, we conduct a case study on text simplification (TS) to investigate how to select the best and most robust examples for ICL. We propose Metric-Based in-context Learning (MBL) method that utilizes commonly used TS metrics such as SARI, compression ratio, and BERT-Precision for selection. Through an extensive set of experiments with various-sized GPT models on standard TS benchmarks such as TurkCorpus and ASSET, we show that examples selected by the top SARI scores perform the best on larger models such as GPT-175B, while the compression ratio generally performs better on smaller models such as GPT-13B and GPT-6.7B. Furthermore, we demonstrate that MBL is generally robust to example orderings and out-of-domain test sets, and outperforms strong baselines and state-of-the-art finetuned language models. Finally, we show that the behavior of large GPT models can be implicitly controlled by the chosen metric. Our research provides a new framework for selecting examples in ICL, and demonstrates its effectiveness in text simplification tasks, breaking new ground for more accurate and efficient NLG systems.",static/papers/inlg/107_Paper.pdf,
inlg,115,inlg115,Exploring the Naturalness of Cognitive Status-Informed Referring Form Selection Models,Gabriel Del Castillo|Grace Clark|Zhao Han|Tom Williams,0,inlgoralsession4,inlg115,"Language-capable robots must be able to efficiently and naturally communicate about objects in the environment. A key part of communication is Referring Form Selection (RFS): the process of selecting a form like it, that, or the N to use when referring to an object. Recent cognitive status-informed computational RFS models have been evaluated in terms of goodness-of-fit to human data. But it is as yet unclear whether these models actually select referring forms that are any more natural than baseline alternatives, regardless of goodness-of-fit. Through a human subject study designed to assess this question, we show that even though cognitive status-informed referring selection models achieve good fit to human data, they do not (yet) produce concrete benefits in terms of naturality. On the other hand, our results show that human utterances also had high variability in perceived naturality, demonstrating the challenges of evaluating RFS naturality.",static/papers/inlg/115_Paper.pdf,
inlg,8,inlg8,System-Initiated Transitions from Chit-Chat to Task-Oriented Dialogues with Transition Info Extractor and Transition Sentence Generator,Ye Liu|Stefan Ultes|Wolfgang Minker|Wolfgang Maier,0,inlgpostersession+flash,inlg8,"In this work, we study dialogue scenarios that start from chit-chat but eventually switch to task-related services, and investigate how a unified dialogue model, which can engage in both chit-chat and task-oriented dialogues, takes the initiative during the dialogue mode transition from chit-chat to task-oriented in a coherent and cooperative manner. We firstly build a \emph{transition info extractor} (TIE) that keeps track of the preceding chit-chat interaction and detects the potential user intention to switch to a task-oriented service. Meanwhile, in the unified model, a \emph{transition sentence generator} (TSG) is extended through efficient Adapter tuning and transition prompt learning. When the TIE successfully finds task-related information from the preceding chit-chat, such as a transition domain (``train'' in Figure \ref{fig: system-initiated transition from chit-chat to task-oriented.}), then the TSG is activated automatically in the unified model to initiate this transition by generating a transition sentence under the guidance of transition information extracted by TIE. The experimental results show promising performance regarding the proactive transitions. We achieve an additional large improvement on TIE model by utilizing Conditional Random Fields (CRF). The TSG can flexibly generate transition sentences while maintaining the unified capabilities of normal chit-chat and task-oriented response generation.",static/papers/inlg/8_Paper.pdf,
inlg,13,inlg13,"HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales",Michele Cafagna|Kees van Deemter|Albert Gatt,0,inlgpostersession+flash,inlg13,"Current captioning datasets focus on object-centric captions, describing the visible objects in the image, often ending up stating the obvious (for humans), e.g. ""people eating food in a park"". Although these datasets are useful to evaluate the ability of Vision & Language models to recognize and describe visual content, they do not support controlled experiments involving model testing or fine-tuning, with more high-level captions, which humans find easy and natural to produce. For example, people often describe images based on the type of scene they depict (""people at a holiday resort"") and the actions they perform (""people having a picnic""). Such concepts are based on personal experience and contribute to forming common sense assumptions. We present the High-Level Dataset, a dataset extending 14997 images from the COCO dataset, aligned with a new set of 134,973 human-annotated (high-level) captions collected along three axes: scenes, actions and rationales. We further extend this dataset with confidence scores collected from an independent set of readers, as well as a set of narrative captions generated synthetically, by combining each of the three axes. We describe this dataset and analyse it extensively. We also present baseline results for the High-Level Captioning task.",static/papers/inlg/13_Paper.pdf,
inlg,25,inlg25,Validating Predictive Models Of Evaluative Language For Controllable Data2Text Generation,Maurice Langner|Ralf Klabunde,0,inlgpostersession+flash,inlg25,"In data2text generation, tabular data is transformed into a text that expresses information from that source domain. While some text types, such as instructions, demand objective and neutral language without any expressive and evaluative content, many other text types are expected to provide expressions for these kinds of subjective meanings.  In controllable, pipelined neural NLG separate learning models, notably regression models, can be used to predict whether some feature deviates sufficiently strongly from an expected value, so that evaluative language would be appropriate for verbalizing this finding. In this paper, we present an empirical study on the comprehension of evaluative adverbs and adjectival modifiers in car reviews, a text type that is characterized by a mixture of factual information with evaluations expressing positive or negative surprise. We show to what extend regression-based decision boundaries for producing evaluative content in controllable data2text NLG match the reader's expectations that are raised by those evaluative markers. Finally we show that regression values in combination with standard deviation of the technical input data constitute reasonable Boolean thresholds for both positive and negative surprise, which provide the basis for the development of more complex models that also include the scalar base of adverbs and modifiers.",static/papers/inlg/25_Paper.pdf,
inlg,27,inlg27,The Next Chapter: A Study of Large Language Models in Storytelling,Zhuohan Xie|Trevor Cohn|Jey Han Lau,0,inlgpostersession+flash,inlg27,"To enhance the quality of generated stories, recent story generation models have been investigating the utilization of higher-level attributes like plots or commonsense knowledge. The application of prompt-based learning with large language models (LLMs), exemplified by GPT-3, has exhibited remarkable performance in diverse natural language processing (NLP) tasks. This paper conducts a comprehensive investigation, utilizing both automatic and human evaluation, to compare the story generation capacity of LLMs with recent models across three  datasets with variations in style, register, and length of stories. The results demonstrate that LLMs generate stories of significantly higher quality compared to other story generation models. Moreover, they exhibit a level of performance that competes with human authors, albeit with the preliminary observation that they tend to replicate real stories in situations involving world knowledge, resembling a form of plagiarism.",static/papers/inlg/27_Paper.pdf,
inlg,28,inlg28,Trustworthiness of Children Stories Generated by Large Language Models,Prabin Bhandari|Hannah Brennan,0,inlgpostersession+flash,inlg28,"Large Language Models (LLMs) have shown a tremendous capacity for generating literary text. However, their effectiveness in generating children's stories has yet to be thoroughly examined. In this study, we evaluate the trustworthiness of children's stories generated by LLMs using various measures, and we compare and contrast our results with both old and new children's stories to better assess their significance. Our findings suggest that LLMs still struggle to generate children's stories at the level of quality and nuance found in actual stories.",static/papers/inlg/28_Paper.pdf,
inlg,50,inlg50,On Text Style Transfer via Style-Aware Masked Language Models,Sharan Narasimhan|Pooja H|Suvodip Dey|Maunendra Sankar Desarkar,0,inlgpostersession+flash,inlg50,"Text Style Transfer (TST) is performable through approaches such as latent space disentanglement, cycle-consistency losses, prototype editing etc. The prototype editing approach, which is known to be quite successful in TST, involves two key phases a) Masking of source style-associated tokens and b) Reconstruction of this source-style masked sentence conditioned with the target style. We follow a similar transduction method, in which we transpose the more difficult direct source to target TST task to a simpler Style-Masked Language Model (SMLM) Task, wherein, similar to BERT \cite{bert}, the goal of our model is now to reconstruct the source sentence from its style-masked version. We arrive at the SMLM mechanism naturally by formulating prototype editing/ transduction methods in a probabilistic framework, where TST resolves into estimating a hypothetical parallel dataset from a partially observed parallel dataset, wherein each domain is assumed to have a common latent style-masked prior. To generate this style-masked prior, we use ""Explainable Attention"" as our choice of attribution for a more precise style-masking step and also introduce a cost-effective and accurate ""Attribution-Surplus"" method of determining the position of masks from any arbitrary attribution model in O(1) time. We empirically show that this non-generational approach well suites the ""content preserving"" criteria for a task like TST, even for a complex style like Discourse Manipulation. Our model, the Style MLM, outperforms strong TST baselines and is on par with state-of-the-art TST models, which use complex architectures and orders of more parameters.",static/papers/inlg/50_Paper.pdf,
inlg,121,inlg121,Affective Natural Language Generation of Event Descriptions through Fine-grained Appraisal Conditions,Yarik Menchaca Resendiz|Roman Klinger,0,inlgpostersession+flash,inlg121,"Models for affective text generation have shown a remarkable progress, but they commonly rely only on basic emotion theories or valance/arousal values as conditions. This is appropriate when the goal is to create explicit emotion statements (""The kid is happy.''). Emotions are, however, commonly communicated implicitly. For instance, the emotional interpretation of an event (""Their dog died.'') does often not require an explicit emotion statement. In psychology, appraisal theories explain the link between a cognitive evaluation of an event and the potentially developed emotion. They put the assessment of the situation on the spot, for instance regarding the own control or the responsibility for what happens. We hypothesize and subsequently show that including appraisal variables as conditions in a generation framework comes with two advantages. (1) The generation model is informed in greater detail about what makes a specific emotion and what properties it has. This leads to text generation that better fulfills the condition. (2) The variables of appraisal allow a user to perform a more fine-grained control of the generated text, by stating properties of a situation instead of only providing the emotion category.  Our Bart and T5-based experiments with 7 emotions (Anger, Disgust, Fear, Guilt, Joy, Sadness, Shame), and 7 appraisals (Attention, Responsibility, Control, Circumstance, Pleasantness, Effort, Certainty) show that (1) adding appraisals during training improves the accurateness of the generated texts by 10 pp in F1. Further, (2) the texts with appraisal variables are longer and contain more details. This exemplifies the greater control for users.",static/papers/inlg/121_Paper.pdf,
inlg,80,inlg80,Leveraging Low-resource Parallel Data for Text Style Transfer,Sourabrata Mukherjee|Ondrej Dusek,0,inlgoralsession2,inlg80,"Text style transfer (TST) involves transforming a text into a desired style while approximately preserving its content. The biggest challenge in TST in the general lack of parallel data. Many existing approaches rely on complex models using substantial non-parallel data, with mixed results. In this paper, we leverage a pretrained BART language model with minimal parallel data and incorporate low-resource methods such as hyperparameter tuning, data augmentation, and self-training, which have not been explored in TST. We further include novel style-based rewards in the training loss. Through extensive experiments in sentiment transfer, a sub-task of TST, we demonstrate that our simple yet effective approaches achieve well-balanced results, surpassing non-parallel approaches and highlighting the usefulness of parallel data even in small amounts.",static/papers/inlg/80_Paper.pdf,
inlg,3,inlg3,Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System,Daphne Ippolito|Nicholas Carlini|Katherine Lee|Milad Nasr|Yun William Yu,0,inlgpostersession+flash,inlg3,"Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse-engineer the decoding method used to generate text (i.e., top-_k_ or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Additionally, the process of discovering the decoding strategy can reveal biases caused by selecting decoding settings which severely truncate a model's predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT).",static/papers/inlg/3_Paper.pdf,
inlg,14,inlg14,Controlling keywords and their positions in text generation,Yuichi Sasazawa|Terufumi Morishita|Hiroaki Ozaki|Osamu Imaichi|Yasuhiro Sogawa,0,inlgpostersession+flash,inlg14,"One of the challenges in text generation is to control text generation as intended by the user. Previous studies proposed specifying the keywords that should be included in the generated text. However, this approach is insufficient to generate text that reflect the user's intent. For example, placing an important keyword at the beginning of the text would help attract the reader's attention; however, existing methods do not enable such flexible control. In this paper, we tackle a novel task of controlling not only keywords but also the position of each keyword in the text generation. To this end, we propose a task-independent method that uses special tokens to control the relative position of keywords. Experimental results on summarization and story generation tasks show that the proposed method can control keywords and their positions. The experimental results also demonstrate that controlling the keyword positions can generate summary texts that are closer to the user's intent than baseline.",static/papers/inlg/14_Paper.pdf,
inlg,26,inlg26,Tackling Hallucinations in Neural Chart Summarization,Saad Obaid ul Islam|Iza Škrjanec|Ondrej Dusek|Vera Demberg,0,inlgpostersession+flash,inlg26,"Hallucinations in text generation occur when the system produces text that is not grounded in the input. In this work, we tackle the problem of hallucinations in neural chart summarization. Our analysis shows that the target side of chart summarization training datasets often contains additional information, leading to hallucinations.  We propose a natural language inference (NLI) based method to preprocess the training data and show through human evaluation that our method significantly reduces hallucinations. We also found that shortening long-distance dependencies in the input sequence and adding chart-related information like title and legends improves the overall performance.",static/papers/inlg/26_Paper.pdf,
inlg,41,inlg41,Learning Disentangled Meaning and Style Representations for Positive Text Reframing,Xu Sheng|Fumiyo Fukumoto|Jiyi Li|Go Kentaro|Yoshimi Suzuki,0,inlgpostersession+flash,inlg41,"The positive text reframing (PTR) task which generates a text giving a positive perspective with preserving the sense of the input text, has attracted considerable attention as one of the NLP applications. Due to the significant representation capability of the pre-trained language model (PLM), a beneficial baseline can be easily obtained by just fine-tuning the PLM. However, how to interpret a diversity of contexts to give a positive perspective is still an open problem. Especially, it is more serious when the size of the training data is limited. In this paper, we present a PTR framework, that learns representations where the meaning and style of text are structurally disentangled. The method utilizes pseudo-positive reframing datasets which are generated with two augmentation strategies. A simple but effective multi-task learning-based model is learned to fuse the generation capabilities from these datasets. Experimental results on Positive Psychology Frames (PPF) dataset, show that our approach outperforms the baselines, BART by five and T5 by six evaluation metrics. Our source codes and data are available online.",static/papers/inlg/41_Paper.pdf,
inlg,59,inlg59,Generating clickbait spoilers with an ensemble of large language models,Mateusz Woźny|Mateusz Lango,0,inlgpostersession+flash,inlg59,"Clickbait posts are a widespread problem in the webspace. The generation of spoilers, i.e. short texts that neutralize clickbait by providing information that makes it uninteresting, is one of the proposed solutions to the problem. Current state-of-the-art methods are based on passage retrieval or question answering approaches and are limited to generating spoilers only in the form of a phrase or a passage. In this work, we propose an ensemble of fine-tuned large language models for clickbait spoiler generation. Our approach is not limited to phrase or passage spoilers, but is also able to generate multipart spoilers that refer to several non-consecutive parts of text. Experimental evaluation demonstrates that the proposed ensemble model outperforms the baselines in terms of BLEU, METEOR and BERTScore metrics.",static/papers/inlg/59_Paper.pdf,
inlg,79,inlg79,Reducing named entity hallucination risk to ensure faithful summary generation,Eunice Akani|Benoit Favre|Frederic Bechet|Romain GEMIGNANI,0,inlgpostersession+flash,inlg79,"The faithfulness of abstractive text summarization at the named entities level is the focus of this study. We propose to add a new criterion to the summary selection method based on the ""risk"" of generating entities that do not belong to the source document. This method is based on the assumption that Out-Of-Document entities are more likely to be hallucinations. This assumption was verified by a manual annotation of the entities occurring in a set of generated summaries on the CNN/DM corpus. This study showed that only 29% of the entities outside the source document were inferrable by the annotators, leading to 71% of hallucinations among OOD entities. We test our selection method on the CNN/DM corpus and show that it significantly reduces the hallucination risk on named entities while maintaining competitive results with respect to automatic evaluation metrics like ROUGE.",static/papers/inlg/79_Paper.pdf,
inlg,82,inlg82,Building a dual dataset of text- and image-grounded conversations and summarisation in Gàidhlig (Scottish Gaelic),David M. Howcroft|William Lamb|Anna Groundwater|Dimitra Gkatzia,0,inlgpostersession+flash,inlg82,"Gàidhlig (Scottish Gaelic; gd) is spoken by about 57k people in Scotland, but remains an under-resourced language with respect to natural language processing in general and natural language generation (NLG) in particular. To address this gap, we developed the first datasets for Scottish Gaelic NLG, collecting both conversational and summarisation data in a single setting. Our task setup involves dialogues between a pair of speakers discussing museum exhibits, grounding the conversation in images and texts. Then, both interlocutors summarise the dialogue resulting in a secondary dialogue summarisation dataset. This paper presents the dialogue and summarisation corpora, as well as the software used for data collection. The corpus consists of 43 conversations (13.7k words) and 61 summaries (2.0k words), and will be released along with the data collection interface.",static/papers/inlg/82_Paper.pdf,
inlg,94,inlg94,Generating Multiple Questions from Presentation Transcripts: A Pilot Study on Earnings Conference Calls,Yining Juan|Chung-Chi Chen|Hen-Hsen Huang|Hsin-Hsi Chen,0,inlgpostersession+flash,inlg94,"In various scenarios, such as conference oral presentations, company managers' talks, and politicians' speeches, individuals often contemplate the potential questions that may arise from their presentations. This common practice prompts the research question addressed in this study: to what extent can models generate multiple questions based on a given presentation transcript? To investigate this, we conduct pilot explorations using earnings conference call transcripts, which serve as regular meetings between professional investors and company managers. We experiment with different task settings and methods and evaluate the results from various perspectives. Our findings highlight that incorporating key points retrieval techniques enhances the accuracy and diversity of the generated questions.",static/papers/inlg/94_Paper.pdf,
inlg,100,inlg100,Mod-D2T: A Multi-layer Dataset for Modular Data-to-Text Generation,Simon Mille|Francois Lareau|Anya Belz|Stamatia Dasiopoulou,0,inlgpostersession+flash,inlg100,"Rule-based text generators lack the coverage and fluency of their neural counterparts, but have two big advantages over them: (i) they are entirely controllable and do not hallucinate; and (ii) they can fully explain how an output was generated from an input. In this paper we leverage these two advantages to create large and reliable synthetic datasets with multiple human-intelligible intermediate representations. We present the Modular Data-to-Text (Mod-D2T) Dataset which incorporates ten intermediate-level representations between input triple sets and output text; the mappings from one level to the next can broadly be interpreted as the traditional modular tasks of an NLG pipeline. We describe the Mod-D2T dataset, evaluate its quality via manual validation and discuss its applications and limitations. Data, code and documentation are available at https://github.com/mille-s/Mod-D2T.",static/papers/inlg/100_Paper.pdf,
sigdial,156,sigdial156,ChatGPT vs. Crowdsourcing vs. Experts: Annotating Open-Domain Conversations with Speech Functions,Lidiia Ostyakova|Veronika Smilga|Kseniia Petukhova|Maria Molchanova|Daniel Kornev,3,sigdialoralsession2,,,,
sigdial,29,sigdial29,Towards Breaking the Self-imposed Filter Bubble in Argumentative Dialogues,Annalena Aicher|Daniel Kornmueller|Yuki Matsuda|Stefan Ultes|Wolfgang Minker|Keiichi Yasumoto,0,sigdialoralsession5,,,,REMOTE
sigdial,74,sigdial74,Opinionconv: A Framework for Generating Opinionated Conversations for Product Search,Vahid Sadiri Javadi|Martin Potthast|Lucie Flek,11,sigdialpostersession1,,,,
sigdial,79,sigdial79,DiactTOD: Learning Generalizable Latent Dialogue Acts for Controllable Task-Oriented Dialogue Systems,Qingyang Wu|James Gung|Raphael Shu|Yi Zhang,0,sigdialoralsession3,,,,REMOTE
sigdial,80,sigdial80,Grounded Complex Task Segmentation for Conversational Assistants,Rafael Ferreira|David Semedo|Joao Magalhaes,3,sigdialoralsession1,,,,
sigdial,86,sigdial86,Sources of Noise in Dialogue and How to Deal with Them,Derek Chen|Zhou Yu,0,sigdialoralsession1,,,,REMOTE
sigdial,81,sigdial81,The Open-domain Paradox for Chatbots: Common Ground as the Basis for Human-like Dialogue,Gabriel Skantze|A. Seza Doğruöz,1,sigdialoralsession5,,,,
sigdial,104,sigdial104,UD_Japanese-CEJC: Dependency Relation Annotation on Corpus of Everyday Japanese Conversation,Mai Omura|Hiroshi Matsuda|Masayuki Asahara|Aya Wakasa,1,sigdialpostersession2,,,,
sigdial,154,sigdial154,"Unravelling Indirect Answers to Wh-Questions: Corpus Construction, Analysis, and Generation",Zulipiye Yusupujiang|Jonathan Ginzburg,10,sigdialpostersession2,,,,
sigdial,15,sigdial15,Approximating Online Human Evaluation of Social Chatbots with Prompting,Ekaterina Svikhnushina|Pearl Pu,1,sigdialoralsession3,,,,
sigdial,97,sigdial97,Dial-M: A Masking-based Framework for Dialogue Evaluation,Suvodip Dey|Maunendra Sankar Desarkar,10,sigdialpostersession1,,,,
sigdial,12,sigdial12,Dialogue Response Generation Using Completion of Omitted Predicate Arguments Based on Zero Anaphora Resolution,Ayaka Ueyama|Yoshinobu Kano,2,sigdialoralsession3,,,,REMOTE
sigdial,129,sigdial129,Syndicom: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback,Christopher Richardson|Larry Heck,3,sigdialoralsession3,,,,
sigdial,14,sigdial14,Investigating Explicitation of Discourse Connectives in Translation using Automatic Annotations,Frances Yung|Merel Scholman|Ekaterina Lapshinova-Koltunski|Christina Pollkläsener|Vera Demberg,1,sigdialoralsession1,,,,
sigdial,43,sigdial43,What's Hard in RST Parsing? Predictive Models for Error Analysis,Yang Janet Liu|Amir Zeldes|Tatsuya Aoyama,2,sigdialoralsession1,,,,
sigdial,91,sigdial91,A Statistical Approach for Quantifying Group Difference in Topic Distributions Using Clinical Discourse Samples,Grace Lawley|Peter A. Heeman|Jill K. Dolata|Eric Fombonne|Steven Bedrick,4,sigdialoralsession1,,,,
sigdial,90,sigdial90,From Chatter to Matter: Addressing Critical Steps of Emotion Recognition Learning in Task-oriented Dialogue,Shutong Feng|Nurul Lubis|Benjamin Ruppik|Christian Geishauser|Michael Heck|Hsien-chin Lin|Carel van Niekerk|Renato Vukovic|Milica Gasic,13,sigdialpostersession1,,,,
sigdial,27,sigdial27,Empathetic Response Generation for Distress Support,Anuradha Welivita|Chun-Hung Yeh|Pearl Pu,3,sigdialoralsession5,,,,
sigdial,73,sigdial73,Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation,Yahui Fu|Koji Inoue|Chenhui Chu|Tatsuya Kawahara,4,sigdialoralsession5,,,,
sigdial,5,sigdial5,Slot Induction via Pre-trained Language Model Probing and Multi-level Contrastive Learning,Hoang Nguyen|Chenwei Zhang|Ye Liu|Philip Yu,2,sigdialoralsession4,,,,REMOTE
sigdial,34,sigdial34,Grounding Description-Driven Dialogue State Trackers with Knowledge-Seeking Turns,Alexandru Coca|Bo-Hsiang Tseng|Jinghong Chen|Weizhe Lin|Weixuan Zhang|Tisha Anders|Bill Byrne,4,sigdialoralsession4,,,,
sigdial,19,sigdial19,Who says what when? Why timing is mission-critical for conversational speech recognition and dialogue systems,Andreas Liesenfeld|Alianda Lopez|Mark Dingemanse,3,sigdialoralsession4,,,,
sigdial,25,sigdial25,Are Large Language Models All You Need for Task-Oriented Dialogue?,Vojtěch Hudeček|Ondrej Dusek,1,sigdialoralsession2,,,,
sigdial,106,sigdial106,Leveraging Large Language Models for Automated Dialogue Analysis,Sarah E. Finch|Ellie S. Paek|Jinho D. Choi,0,sigdialoralsession2,,,,REMOTE
sigdial,109,sigdial109,"Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering",Angus Addlesee|Weronika Sieińska|Nancie Gunson|Daniel Hernandez Garcia|Christian Dondrup|Oliver Lemon,2,sigdialoralsession2,,,,
sigdial,56,sigdial56,Analyzing Differences in Subjective Annotations by Participants and Third-party Annotators in Multimodal Dialogue Corpus,Kazunori Komatani|Ryu Takeda|Shogo Okada,7,sigdialpostersession1,,,,
sigdial,49,sigdial49,Learning Multimodal Cues of Children's Uncertainty,Qi Cheng|Mert Inan|Rahma Mbarki|Theresa Choi|Yiming Sun|Kimele Persaud|Jenny Wang|Malihe Alikhani,0,sigdialoralsession4,,,,REMOTE
sigdial,122,sigdial122,Resolving References in Visually-Grounded Dialogue via Text Generation,Bram Willemsen|Livia Qian|Gabriel Skantze,1,sigdialoralsession4,,,,
sigdial,130,sigdial130,MERCY: Multiple Response Ranking Concurrently in Realistic Open-Domain Conversational Systems,Sarik Ghazarian|Behnam Hedayatnia|Di Jin|Sijia Liu|Nanyun Peng|Yang Liu|Dilek Hakkani-Tur,2,sigdialoralsession5,,,,REMOTE
sigdial,163,sigdial163,"""What do others think?"": Task-Oriented Conversational Modeling with Subjective Knowledge",Chao Zhao|Spandana Gella|Seokhwan Kim|Di Jin|Devamanyu Hazarika|Alexandros Papangelis|Behnam Hedayatnia|Mahdi Namazifar|Yang Liu|Dilek Hakkani-Tur,4,sigdialoralsession3,,,,
sigdial,54,sigdial54,Enhancing Task Bot Engagement with Synthesized Open-Domain Dialog,Miaoran Li|Baolin Peng|Michel Galley|Jianfeng Gao|Zhu (Drew) Zhang,1,virtualpostersession,,,,
sigdial,67,sigdial67,Frame-oriented Summarization of Argumentative Discussions,Shahbaz Syed|Timon Ziegenbein|Philipp Heinisch|Henning Wachsmuth|Martin Potthast,9,sigdialpostersession1,,,,
sigdial,153,sigdial153,A New Dataset for Causality Identification in Argumentative Texts,Khalid Al Khatib|Michael Völske|Anh Le|Shahbaz Syed|Martin Potthast|Benno Stein,9,sigdialpostersession2,,,,
sigdial,125,sigdial125,Controllable Generation of Dialogue Acts for Dialogue Systems via Few-Shot Response Generation and Ranking,Angela Ramirez|Kartik Agarwal|Juraj Juraska|Utkarsh Garg|Marilyn Walker,5,sigdialpostersession2,,,,
sigdial,117,sigdial117,Reference Resolution and New Entities in Exploratory Data Visualization: From Controlled to Unconstrained Interactions with a Conversational Assistant,Abari Bhattacharya|Abhinav Kumar|Barbara Di Eugenio|Roderick Tabalba|Jillian Aurisano|Veronica Grosso|Andrew Johnson|Jason Leigh|Moira Zellner,2,sigdialpostersession2,,,,
sigdial,20,sigdial20,Towards Multilingual Automatic Open-Domain Dialogue Evaluation,John Mendonca|Alon Lavie|Isabel Trancoso,2,sigdialpostersession1,,,,
sigdial,40,sigdial40,Dialog Action-Aware Transformer for Dialog Policy Learning,Huimin Wang|Wai Chung Kwan|Kam-Fai Wong,4,sigdialpostersession1,,,,
sigdial,75,sigdial75,The Wizard of Curiosities: Enriching Dialogues with Epistemic Fun-facts,Frederico Vicente|Rafael Ferreira|David Semedo|Joao Magalhaes,12,sigdialpostersession1,,,,
sigdial,46,sigdial46,Transformer-based Multi-Party Conversation Generation using Dialogue Discourse Acts Planning,Alexander Chernyavskiy|Dmitry Ilvovsky,7,virtualpostersession,,,,
sigdial,148,sigdial148,CONVERSER: Few-shot Conversational Dense Retrieval with Synthetic Data Generation,Chao-Wei Huang|Chen-Yu Hsu|Tsu-Yuan Hsu|Chen-An Li|Yun-Nung Chen,8,sigdialpostersession2,,,,
sigdial,128,sigdial128,Incorporating annotators' uncertainty to discourse relations representations,S. Magalí López Cortez|Cassandra L. Jacobs,0,virtualpostersession,,,,
sigdial,35,sigdial35,The Road to Quality is Paved with Good Revisions: A Detailed Evaluation Methodology for Revision Policies in Incremental Sequence Labelling,Brielen Madureira|Patrick Kahardipraja|David Schlangen,3,sigdialpostersession1,,,,
sigdial,4,sigdial4,The effect of conversation type on entrainment: Evidence from laughter,Bogdan Ludusan|Petra Wagner,1,sigdialpostersession1,,,,
sigdial,30,sigdial30,Investigating the Representation of Open Domain Dialogue Context for Transformer Models,Vishakh Padmakumar|Behnam Hedayatnia|Di Jin|Patrick Lange|Seokhwan Kim|Nanyun Peng|Yang Liu|Dilek Hakkani-Tur,4,virtualpostersession,,,,
sigdial,123,sigdial123,Speaker Role Identification in Call Center Dialogues: Leveraging Opening Sentences and Large Language Models,Minh-Quoc Nghiem|Nichola Roberts|Dmitry Sityaev,4,sigdialpostersession2,,,,
sigdial,42,sigdial42,C3: Compositional Counterfactual Constrastive Learning for Video-grounded Dialogues,Hung Le|Nancy Chen|Steven C.H. Hoi,5,virtualpostersession,,,,
sigdial,50,sigdial50,‘What are you referring to?' Evaluating the ability of multi-modal dialogue models to process clarificational exchanges,Javier Chiyah-Garcia|Alessandro Suglia|Arash Eshghi|Helen Hastie,6,sigdialpostersession1,,,,
sigdial,3,sigdial3,PGTask: Introducing the Task of Profile Generation from Dialogues,Rui Ribeiro|Joao Paulo Carvalho|Luisa Coheur,0,sigdialpostersession1,,,,
sigdial,146,sigdial146,Synthesising Personality with Neural Speech Synthesis,Shilin Gao|Matthew P. Aylett|David A. Braude|Catherine Lai,7,sigdialpostersession2,,,,
sigdial,59,sigdial59,No that's not what I meant: Handling Third Position Repair in Conversational Question Answering,Vevake Balaraman|Arash Eshghi|Ioannis Konstas|Ioannis Papaioannou,8,virtualpostersession,,,,
sigdial,61,sigdial61,Question Generation to Elicit Users' Food Preferences by Considering the Semantic Content,Jie Zeng|Yukiko Nakano|Tatsuya Sakato,8,sigdialpostersession1,,,,
sigdial,138,sigdial138,Improve Zero-shot Performance on Unseen Dialog Domains with Retrieval-Augmented Task-Oriented Systems,Jianguo Zhang|Stephen Roller|Kun Qian|Zhiwei Liu|Rui Meng|Shelby Heinecke|Huan Wang|silvio savarese|Caiming Xiong,2,virtualpostersession,,,,
sigdial,121,sigdial121,"Prompting, Retrieval, Training: An exploration of different approaches for task-oriented dialogue generation",Gonçalo Raposo|Luisa Coheur|Bruno Martins,3,sigdialpostersession2,,,,
sigdial,132,sigdial132,Bootstrapping a Conversational Guide for Colonoscopy Prep,Pulkit Arya|Madeleine Bloomquist|SUBHANKAR CHAKRABORTY|Andrew Perrault|William Schuler|Eric Fosler-Lussier|Michael White,6,sigdialpostersession2,,,,
sigdial,155,sigdial155,Applying Item Response Theory to Task-oriented Dialogue Systems for Accurately Determining User's Task Success Ability,Ryu Hirai|Ao Guo|Ryuichiro Higashinaka,11,sigdialpostersession2,,,,
sigdial,147,sigdial147,When to generate hedge in peer tutoring interaction?,Alafate Abulimiti|Chloé Clavel|Justine Cassell,3,virtualpostersession,,,,
sigdial,45,sigdial45,PaperPersiChat: Scientific Paper Discussion Chatbot using Transformers and Discourse Flow Management,Alexander Chernyavskiy|Max Bregeda|Maria Nikiforova,6,virtualpostersession,,,,
sigdial,48,sigdial48,Roll Up Your Sleeves: Working with a Collaborative and Engaging Task-Oriented Dialogue System,Lingbo Mo|Shijie Chen|Ziru Chen|Xiang Deng|Ashley Lewis|Sunit Singh|Samuel Stevens|Chang-You Tai|Zhen Wang|Xiang Yue|Tianshu Zhang|Yu Su|Huan Sun,5,sigdialpostersession1,,,,
sigdial,70,sigdial70,An Open-Domain Avatar Chatbot by Exploiting a Large Language Model,Takato Yamazaki|Tomoya Mizumoto|Katsumasa Yoshikawa|Masaya Ohagi|Toshiki Kawamoto|Toshinori Sato,0,sigdialpostersession2,,,,
sigdial,103,sigdial103,"FurChat: An Embodied Conversational Agent using LLMs, Combining Open and Closed-Domain Dialogue with Facial Expressions",Neeraj Cherakara|Finny Varghese|Sheena Shabana|Nivan Nelson|Abhiram Karukayil|Rohith Kulothungan|Mohammed Afil Farhan|Birthe Nesset|Meriam Moujahid|Tanvi Dinkar|Verena Rieser|Oliver Lemon,9,virtualpostersession,,,,
sigdial,D&D1,sigdialD&D1,Studying Alignment in a Collaborative Learning Activity via Automatic Methods: The Link Between What We Say and Do,Utku Norman|Tanvi Dinkar|Barbara Bruno|Chloé Clavel,12,sigdialpostersession2,,,,
sigdial,D&D2,sigdialD&D2,The effect of domain knowledge on discourse relation inferences: Relation marking and interpretation strategies,Marian Marchal|Saarland University|Merel C.J. Scholman|Vera Demberg,13,sigdialpostersession2,,,,
sigdial,D&D3,sigdialD&D3,"Bullshit, Pragmatic Deception, and Natural Language Processing",Oliver Deck,14,sigdialpostersession2,,,,
inlg,130,inlg130,A Report on FCG GenChal 2022: Shared Task on Feedback Comment Generation for Language Learners,"Ryo Nagata, Masato Hagiwara, Kazuaki Hanawa and Masato Mita",0,inlggenchalpresentation,130,,,
inlg,136,inlg136,AIWolfDial 2023: Summary of Natural Language Division of 5th International AIWolf Contest,"Yoshinobu Kano, Neo Watanabe, Kaito Kagaminuma, Claus Aranha, Jaewon Lee, Benedek Hauer, Hisaichi Shibata, Soichiro Miki, Yuta Nakamura and Takuya Okubo",1,inlggenchalpresentation,136,,,
inlg,142,inlg142,Overview of the Second Shared Task on Automatic Minuting (AutoMin) at INLG 2023,"Tirthankar Ghosal, Ondřej Bojar, Marie Hledíková, Tom Kocmi and Anna Nedoluzhko",2,inlggenchalpresentation,142,,,
inlg,116,inlg116,LOWRECORP: the Low-Resource NLG Corpus Building Challenge,"Khyathi Raghavi Chandu, David M. Howcroft, Dimitra Gkatzia, Yi-ling Chung, Yufang Hou, Chris Chinenye Emezue, Pawan Rajpoot and Tosin Adewumi",3,inlggenchalpresentation,116,,,
inlg,117,inlg117,Long Story Generation Challenge,Nikolay Mikhaylovskiy,4,inlggenchalpresentation,117,,,
inlg,119,inlg119,Visually Grounded Story Generation Challenge,"Xudong Hong, Khushboo Mehra, Asad Sayeed and Vera Demberg",5,inlggenchalpresentation,119,,,
inlg,120,inlg120,The VDG Challenge: Response Generation and Evaluation in Collaborative Visual Dialogue,Nikolai Ilinykh and Simon Dobnik,6,inlggenchalpresentation,120,,,
inlg,132,inlg132,"Retrieval, Masking, and Generation: Feedback Comment Generation using Masked Comment Examples","Mana Ihori, Hiroshi Sato, Tomohiro Tanaka and Ryo Masumura",0,genchalpostersession+demos,132,,,
inlg,138,inlg138,Team Synapse @ AutoMin 2023: Leveraging BART-Based Models for Automatic Meeting Minuting,Kristýna Klesnilová and Michelle Elizabeth,1,genchalpostersession+demos,138,,,
inlg,139,inlg139,Team Iterate @ AutoMin 2023 - Experiments with Iterative Minuting,František Kmječ and Ondřej Bojar,2,genchalpostersession+demos,139,,,
inlg,140,inlg140,Darbarer@AutoMin2023: Transcription simplification for concise minute generation from multi-party conversations,"Ismaël Rousseau, Loïc Fosse, Youness Dkhissi, Geraldine Damnati and Gwénolé Lecorvé",3,genchalpostersession+demos,140,,,
inlg,133,inlg133,TMU Feedback Comment Generation System Using Pretrained Sequence-to-Sequence Language Models,Naoya Ueda and Mamoru Komachi,0,virtualpostersession,133,,,
inlg,137,inlg137,Team Zoom @ AutoMin 2023: Utilizing Topic Segmentation And LLM Data Augmentation For Long-Form Meeting Summarization,Felix Schneider and Marco Turchi,1,virtualpostersession,137,,,
inlg,141,inlg141,"Team NTR @ AutoMin 2023: Dolly LLM Improves Minuting Performance, Semantic Segmentation Doesn’t",Eugene Borisov and Nikolay Mikhaylovskiy,2,virtualpostersession,141,,,
