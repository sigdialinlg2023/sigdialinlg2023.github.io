

<!-- venue -->
<!-- Venue na webu sigdialu participating -->
<!-- nahradit fotku a zmergovat logo -->
<!DOCTYPE html>
<html lang="en">

<head>
  
<script type="text/javascript" src="static/js/views/zoom_links.js"></script>


  <!-- Required meta tags -->
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

  <!-- External Javascript libs_ext  -->
  <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

  <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
    integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
    integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
    integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct"
    crossorigin="anonymous"></script>
  <!-- <script src="https://cdn.jsdelivr.net/npm/bootstrap-select@1.14.0-beta3/dist/js/bootstrap-select.min.js"></script>

  <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
    integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
    integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script> -->


  <!-- Library libs_ext -->
  <script src="static/js/libs_ext/typeahead.bundle.js"></script>


  <!--    Internal Libs -->
  <script src="static/js/data/api.js"></script>

  
  <script>
    var auth0_domain = "ufal-cuni.eu.auth0.com";
    var auth0_client_id = "R6G028fWc6YPC89JqBds3RaPO4SgZPkO";
  </script>
  <script src="https://cdn.auth0.com/js/auth0-spa-js/2.0/auth0-spa-js.production.js">
  </script>
  <script src="static/js/modules/auth0protect.js"></script>
  

  <!-- External CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css"
    integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">


  <!-- External Fonts (no google for china) -->
  <link href="static/css/Lato.css" rel="stylesheet" />
  <!-- <link href="static/css/Exo.css" rel="stylesheet" /> -->
  <link href="https://fonts.googleapis.com/css2?family=Exo:wght@400;700&family=Lato:wght@400;700&display=swap"
    rel="stylesheet">
  <link href="static/css/Cuprum.css" rel="stylesheet" />
  <link rel="stylesheet" href="static/css/main.css" />
  <!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
  <link rel="stylesheet" href="static/css/fa_solid.css" />
  <link rel="stylesheet" href="static/css/lazy_load.css" />
  <link rel="stylesheet" href="static/css/typeahead.css" />

  <title>Program SIGdial &amp; INLG 2023: Slot Induction via Pre-Trained Language Model Probing and Multi-Level Contrastive Learning</title>
  
<meta name="citation_title" content="Slot Induction via Pre-Trained Language Model Probing and Multi-Level Contrastive Learning" />

<meta name="citation_author" content="Hoang Nguyen" />

<meta name="citation_author" content="Chenwei Zhang" />

<meta name="citation_author" content="Ye Liu" />

<meta name="citation_author" content="Philip Yu" />

<meta name="citation_publication_date" content="September 2023" />
<meta name="citation_conference_title" content="Visit Sigdial &amp; Inlg 2023" />
<meta name="citation_inbook_title" content="Proceedings of SIGdial 2023 &amp; Proceedings of INLG 2023" />
<meta name="citation_abstract" content="Recent advanced methods in Natural Language Understanding for Task-oriented Dialogue (TOD) Systems (e.g., intent detection and slot filling) require a large amount of annotated data to achieve competitive performance. In reality, token-level annotations (slot labels) are time-consuming and difficult to acquire. In this work, we study the Slot Induction (SI) task whose objective is to induce slot boundaries without explicit knowledge of token-level slot annotations. We propose leveraging Unsupervised Pre-trained Language Model (PLM) Probing and Contrastive Learning mechanism to exploit (1) unsupervised semantic knowledge extracted from PLM, and (2) additional sentence-level intent label signals available from TOD. Our approach is shown to be effective in SI task and capable of bridging the gaps with token-level supervised models on two NLU benchmark datasets. When generalized to emerging intents, our SI objectives also provide enhanced slot label representations, leading to improved performance on the Slot Filling tasks." />



</head>

<body>
  <!-- NAV -->
  <!-- TODO TBD program -->
  
  
  
  
  

  <nav class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto" id="main-nav">
    <div class="container">
      <a class="navbar-brand" href="index.html">
        <img class="logo" src="static/images/program.png" height="auto"
          width="170px" />
      </a>
      
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item active">
            <a class="nav-link" href="index.html">News</a>
          </li>

          
            
            

          
            
            





          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown"
              aria-haspopup="true" aria-expanded="false">
              For participants
            </a>
            <div class="dropdown-menu" aria-labelledby="navbarDropdown">
              <a class="dropdown-item" href="registration.html">Registration</a>
              <a class="dropdown-item" href="venue.html">Venue</a>
              <a class="dropdown-item" href="onlinepresence.html">Online Presence</a>
              <a class="dropdown-item" href="local.html">Local information</a>
              <a class="dropdown-item" href="presenters.html">For presenters</a>
            </div>
          </li>

          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown"
              aria-haspopup="true" aria-expanded="false">
              Program
            </a>
            <div class="dropdown-menu" aria-labelledby="navbarDropdown">
              <a class="dropdown-item" href="calendar.html">Schedule</a>
              <a class="dropdown-item" href="speakers.html">Keynotes</a>
              <!--                    <a class="dropdown-item" href="panels.html">Panel</a>-->
              <!--                    <a class="dropdown-item" href="accepted_papers.html">Accepted Papers</a>-->
              <a class="dropdown-item" href="papers.html">Papers</a>
              <a class="dropdown-item" href="awards.html">Awards</a>
              <a class="dropdown-item" href="workshops.html">Workshops</a>
              <!--                    <a class="dropdown-item" href="tutorials.html">Tutorials</a>-->
              <!--                    <a class="dropdown-item" href="hackathons.html">Hackathon</a>-->
              <!--                    <a class="dropdown-item" href="genchal.html">GenChal</a>-->
            </div>
          </li>



          <li class="nav-item ">
            <a class="nav-link" href="help.html">FAQ</a>
          </li>

          <li class="nav-item ">
            <a class="nav-link" href="organizers.html">Organizers</a>
          </li>

          <li class="nav-item ">
            <a class="nav-link" href="sponsors.html">Sponsors</a>
          </li>

          <!-- <li class="nav-item "> -->
          <!--     <a class="nav-link" href="workshops.html">Workshops</a> -->
          <!-- </li> -->

          <li class="nav-item ">
            <a class="nav-link" href="https://2023.sigdial.org/">SIGdial</a>
          </li>

          <li class="nav-item ">
            <a class="nav-link" href="https://inlg2023.github.io/">INLG</a>
          </li>
        </ul>

        <button id="btn-login" class="btn-info btn-sm btn-login" onclick="login()">Log in</button>
        <button id="btn-logout" class="btn-danger btn-sm btn-login" onclick="logout()">Log out</button>
      </div>
    </div>
  </nav>
  

  
  <!-- User Overrides -->
   
  <div class="container">
    <!-- Tabs -->
    <div class="tabs">
       
    </div>

    <!-- Content -->
    <div class="content">
      <div id="error-message" class="error-message"></div>
      

<!-- Title -->
<!-- <div class="public-content"> -->
<div class="pp-card m-3">
    <div class="card-header">
        <h2 class="card-title main-title text-center">
            Slot Induction via Pre-Trained Language Model Probing and Multi-Level Contrastive Learning
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-center">
            
            <a href="papers.html?filter=authors&search=Hoang Nguyen" class="text-muted"><i>Hoang Nguyen</i></a>,
            
            <a href="papers.html?filter=authors&search=Chenwei Zhang" class="text-muted"><i>Chenwei Zhang</i></a>,
            
            <a href="papers.html?filter=authors&search=Ye Liu" class="text-muted"><i>Ye Liu</i></a>,
            
            <a href="papers.html?filter=authors&search=Philip Yu" class="text-muted"><i>Philip Yu</i></a>
            
        </h3>
        <div class="text-center p-3">
            
            <p>
                <a class="card-link" target="_blank" href="static/papers/sigdial/5_Paper.pdf"> Paper </a>
            </p>
            

            In Sessions:
            
            <!-- {&#39;UID&#39;: &#39;sigdialoralsession4&#39;, &#39;title&#39;: &#39;Sigdial Oral Session 4: Language understanding and multimodality&#39;, &#39;chair&#39;: &#39;Tatsuya Kawahara&#39;, &#39;start&#39;: &#39;2023-09-14T16:10:00+02:00&#39;, &#39;room&#39;: &#39;Sun I&#39;, &#39;category&#39;: &#39;time&#39;, &#39;end&#39;: &#39;2023-09-14T17:50:00+02:00&#39;, &#39;discord&#39;: &#39;https://discord.com/channels/###sigdialoralsession4###&#39;, &#39;zoom&#39;: &#39;https://zoom.us/j/###Sun I###&#39;, &#39;calendarId&#39;: &#39;sigdialoral&#39;, &#39;contents&#39;: [{&#39;conference&#39;: &#39;sigdial&#39;, &#39;original_id&#39;: &#39;49&#39;, &#39;UID&#39;: &#39;sigdial49&#39;, &#39;title&#39;: &#34;Learning Multimodal Cues of Children&#39;s Uncertainty&#34;, &#39;authors&#39;: [&#39;Qi Cheng&#39;, &#39;Mert Inan&#39;, &#39;Rahma Mbarki&#39;, &#39;Theresa Choi&#39;, &#39;Yiming Sun&#39;, &#39;Kimele Persaud&#39;, &#39;Jenny Wang&#39;, &#39;Malihe Alikhani&#39;], &#39;order&#39;: &#39;0&#39;, &#39;session&#39;: &#39;sigdialoralsession4&#39;, &#39;abstract&#39;: &#39;Understanding uncertainty plays a critical role in achieving common ground (Clark et al., 1983). This is especially important for multimodal AI systems that collaborate with users to solve a problem or guide the user through a challenging concept.  In this work, for the first time, we present a dataset annotated in collaboration with developmental and cognitive psychologists for the purpose of studying nonverbal cues of uncertainty. We then present an analysis of the data, studying different roles of uncertainty and its relationship with task difficulty and performance. Lastly, we present a multimodal machine learning model that can predict uncertainty given a real-time video clip of a participant, which we find improves upon a baseline multimodal transformer model. This work informs research on cognitive coordination between human-human and human-AI and has broad implications for gesture understanding and generation.  The anonymized version of our data and code will be publicly available upon the completion of the required consent forms and data sheets.&#39;, &#39;paper&#39;: &#39;static/papers/sigdial/49_Paper.pdf&#39;, &#39;notes&#39;: &#39;REMOTE&#39;, &#39;poster&#39;: &#39;&#39;, &#39;full_video&#39;: &#39;&#39;, &#39;start&#39;: &#39;2023-09-14T16:10:00+02:00&#39;}, {&#39;conference&#39;: &#39;sigdial&#39;, &#39;original_id&#39;: &#39;34&#39;, &#39;UID&#39;: &#39;sigdial34&#39;, &#39;title&#39;: &#39;Grounding Description-Driven Dialogue State Trackers With Knowledge-Seeking Turns&#39;, &#39;authors&#39;: [&#39;Alexandru Coca&#39;, &#39;Bo-Hsiang Tseng&#39;, &#39;Jinghong Chen&#39;, &#39;Weizhe Lin&#39;, &#39;Weixuan Zhang&#39;, &#39;Tisha Anders&#39;, &#39;Bill Byrne&#39;], &#39;order&#39;: &#39;1&#39;, &#39;session&#39;: &#39;sigdialoralsession4&#39;, &#39;abstract&#39;: &#39;Schema-guided dialogue state trackers can generalise to new domains without further training, yet they are sensitive to the writing style of the schemata. Augmenting the training set with human or synthetic schema paraphrases improves the model robustness to these variations but can be either costly or difficult to control.  We propose to circumvent these issues by grounding the state tracking model in knowledge-seeking turns collected from the dialogue corpus as well as the schema. Including these turns in prompts during finetuning and inference leads to marked improvements in model robustness, as demonstrated by large average joint goal accuracy and schema sensitivity improvements on SGD and SGD-X.&#39;, &#39;paper&#39;: &#39;static/papers/sigdial/34_Paper.pdf&#39;, &#39;notes&#39;: &#39;Best Paper Nominee&#39;, &#39;poster&#39;: &#39;&#39;, &#39;full_video&#39;: &#39;&#39;, &#39;start&#39;: &#39;2023-09-14T16:30:00+02:00&#39;}, {&#39;conference&#39;: &#39;sigdial&#39;, &#39;original_id&#39;: &#39;122&#39;, &#39;UID&#39;: &#39;sigdial122&#39;, &#39;title&#39;: &#39;Resolving References in Visually-Grounded Dialogue via Text Generation&#39;, &#39;authors&#39;: [&#39;Bram Willemsen&#39;, &#39;Livia Qian&#39;, &#39;Gabriel Skantze&#39;], &#39;order&#39;: &#39;2&#39;, &#39;session&#39;: &#39;sigdialoralsession4&#39;, &#39;abstract&#39;: &#39;Vision-language models (VLMs) have shown to be effective at image retrieval based on simple text queries, but text-image retrieval based on conversational input remains a challenge. Consequently, if we want to use VLMs for reference resolution in visually-grounded dialogue, the discourse processing capabilities of these models need to be augmented. To address this issue, we propose fine-tuning a causal large language model (LLM) to generate definite descriptions that summarize coreferential information found in the linguistic context of references. We then use a pretrained VLM to identify referents based on the generated descriptions, zero-shot. We evaluate our approach on a manually annotated dataset of visually-grounded dialogues and achieve results that, on average, exceed the performance of the baselines we compare against. Furthermore, we find that using referent descriptions based on larger context windows has the potential to yield higher returns.&#39;, &#39;paper&#39;: &#39;static/papers/sigdial/122_Paper.pdf&#39;, &#39;notes&#39;: &#39;&#39;, &#39;poster&#39;: &#39;&#39;, &#39;full_video&#39;: &#39;&#39;, &#39;start&#39;: &#39;2023-09-14T16:50:00+02:00&#39;}, {&#39;conference&#39;: &#39;sigdial&#39;, &#39;original_id&#39;: &#39;5&#39;, &#39;UID&#39;: &#39;sigdial5&#39;, &#39;title&#39;: &#39;Slot Induction via Pre-Trained Language Model Probing and Multi-Level Contrastive Learning&#39;, &#39;authors&#39;: [&#39;Hoang Nguyen&#39;, &#39;Chenwei Zhang&#39;, &#39;Ye Liu&#39;, &#39;Philip Yu&#39;], &#39;order&#39;: &#39;3&#39;, &#39;session&#39;: &#39;sigdialoralsession4&#39;, &#39;abstract&#39;: &#39;Recent advanced methods in Natural Language Understanding for Task-oriented Dialogue (TOD) Systems (e.g., intent detection and slot filling) require a large amount of annotated data to achieve competitive performance. In reality, token-level annotations (slot labels) are time-consuming and difficult to acquire. In this work, we study the Slot Induction (SI) task whose objective is to induce slot boundaries without explicit knowledge of token-level slot annotations. We propose leveraging Unsupervised Pre-trained Language Model (PLM) Probing and Contrastive Learning mechanism to exploit (1) unsupervised semantic knowledge extracted from PLM, and (2) additional sentence-level intent label signals available from TOD. Our approach is shown to be effective in SI task and capable of bridging the gaps with token-level supervised models on two NLU benchmark datasets. When generalized to emerging intents, our SI objectives also provide enhanced slot label representations, leading to improved performance on the Slot Filling tasks.&#39;, &#39;paper&#39;: &#39;static/papers/sigdial/5_Paper.pdf&#39;, &#39;notes&#39;: &#39;REMOTE&#39;, &#39;poster&#39;: &#39;static/posters/SIGDIAL2023/5.pdf&#39;, &#39;full_video&#39;: &#39;&#39;, &#39;start&#39;: &#39;2023-09-14T17:10:00+02:00&#39;}, {&#39;conference&#39;: &#39;sigdial&#39;, &#39;original_id&#39;: &#39;19&#39;, &#39;UID&#39;: &#39;sigdial19&#39;, &#39;title&#39;: &#39;The Timing Bottleneck: Why Timing and Overlap Are Mission-Critical for Conversational User Interfaces, Speech Recognition and Dialogue Systems&#39;, &#39;authors&#39;: [&#39;Andreas Liesenfeld&#39;, &#39;Alianda Lopez&#39;, &#39;Mark Dingemanse&#39;], &#39;order&#39;: &#39;4&#39;, &#39;session&#39;: &#39;sigdialoralsession4&#39;, &#39;abstract&#39;: &#39;Speech recognition systems are a key intermediary in voice-driven human-computer interaction. Although speech recognition works well for pristine monologic audio, real-life use cases in open-ended interactive settings still present many challenges. We argue that timing is mission-critical for dialogue systems, and evaluate 5 major commercial ASR systems for their conversational and multilingual support. We find that word error rates for natural conversational data in 6 languages remain abysmal, and that overlap remains a key challenge (study 1). This impacts especially the recognition of conversational words (study 2), and in turn has dire consequences for downstream intent recognition (study 3). Our findings help to evaluate the current state of conversational ASR, contribute towards multidimensional error analysis and evaluation, and identify phenomena that need most attention on the way to build robust interactive speech technologies.&#39;, &#39;paper&#39;: &#39;static/papers/sigdial/19_Paper.pdf&#39;, &#39;notes&#39;: &#39;Best Paper Nominee&#39;, &#39;poster&#39;: &#39;&#39;, &#39;full_video&#39;: &#39;&#39;, &#39;start&#39;: &#39;2023-09-14T17:30:00+02:00&#39;}], &#39;name&#39;: &#39;Sigdial Oral Session 4: Language understanding and multimodality&#39;, &#39;start_time&#39;: datetime.datetime(2023, 9, 14, 16, 10, tzinfo=datetime.timezone(datetime.timedelta(seconds=7200))), &#39;end_time&#39;: datetime.datetime(2023, 9, 14, 17, 50, tzinfo=datetime.timezone(datetime.timedelta(seconds=7200))), &#39;location&#39;: &#39;#sigdialoralsession4&#39;, &#39;day&#39;: &#39;Thursday&#39;} - <br /> -->
            <p>
                Sigdial Oral Session 4: Language understanding and multimodality: (Thursday, 16:10 CEST, Sun I
                <span class="gated-content">
                    , <a href="https://zoom.us/j/###Sun I###" target="_blank"
                        class="card-link">Watch on Zoom</a> 
                    , <a href="https://discord.com/channels/###sigdialoralsession4###" target="_blank"
                        class="card-link discord-link">Chat on Discord </a>
                </span>
                )
            </p>
            

            
            
            <a href="static/posters/SIGDIAL2023/5.pdf" target="_blank" class="card-link">
                <p>Poster </p>
                <img src="static/posters/thumbnails/sigdial5.png" alt="Slot Induction via Pre-Trained Language Model Probing and Multi-Level Contrastive Learning">
            </a>
            
            
            <br />
            
        </div>
    </div>
</div>

<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <span class="font-weight-bold">Abstract:</span>
                Recent advanced methods in Natural Language Understanding for Task-oriented Dialogue (TOD) Systems (e.g., intent detection and slot filling) require a large amount of annotated data to achieve competitive performance. In reality, token-level annotations (slot labels) are time-consuming and difficult to acquire. In this work, we study the Slot Induction (SI) task whose objective is to induce slot boundaries without explicit knowledge of token-level slot annotations. We propose leveraging Unsupervised Pre-trained Language Model (PLM) Probing and Contrastive Learning mechanism to exploit (1) unsupervised semantic knowledge extracted from PLM, and (2) additional sentence-level intent label signals available from TOD. Our approach is shown to be effective in SI task and capable of bridging the gaps with token-level supervised models on two NLU benchmark datasets. When generalized to emerging intents, our SI objectives also provide enhanced slot label representations, leading to improved performance on the Slot Filling tasks.
            </div>
        </div>
        <p></p>
    </div>
</div>
<!-- </div> -->


<script type="text/javascript">
    window.addEventListener("load", updateLinks(), "false" );
</script>

    </div>
  </div>
  
  

  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag("js", new Date());
    gtag("config", "UA-");
  </script>

  <!-- Footer -->
  <footer class="footer bg-light p-4">
    <div class="container">
      <div class="gated-content" style="display: none;">
        <p class="float-left">
          Logged in as <b><span id="ipt-user-email"></span></b>.</p>
      </div>
      <p class="float-right"><a href="#">Back to Top</a></p>
      <p class="text-center">© 2023 SIGDIAL-INLG 2023 Organizers</p>
    </div>
  </footer>

  <!-- Code for hash tags -->
  <script type="text/javascript">
    $(document).ready(function () {
      if (window.location.hash !== "") {
        $(`a[href="${window.location.hash}"]`).tab("show");
      }

      $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
        const hash = $(e.target).attr("href");
        if (hash.substr(0, 1) === "#") {
          const position = $(window).scrollTop();
          window.location.replace(`#${hash.substr(1)}`);
          $(window).scrollTop(position);
        }
      });
    });
  </script>
  <!--    <script src="static/js/modules/lazyLoad.js"></script>-->
  
</body>

</html>